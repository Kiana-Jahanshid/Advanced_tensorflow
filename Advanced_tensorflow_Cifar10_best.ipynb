{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D708XPkBW6_m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense , Flatten , Conv2D , MaxPooling2D , Dropout , BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gt4RYO54XWUn"
      },
      "outputs": [],
      "source": [
        "CIFAR10 = keras.datasets.cifar10\n",
        "\n",
        "(x_train , y_train) , (x_test , y_test ) = CIFAR10.load_data()\n",
        "x_train , x_test = x_train/255.0 , x_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g02f9IZWYIeW",
        "outputId": "d079118f-87cc-45df-f75c-79d67d23c53b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x_train[2].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL_ReR_QYZCT"
      },
      "source": [
        "# Adding batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vykaPmm7YW5g"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train , y_train)).batch(20).shuffle(buffer_size=1000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test , y_test)).batch(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D8vtlPejOAw",
        "outputId": "5ba98340-64ad-4104-97d6-c05a97e1f88e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkEsoDp9TdO_"
      },
      "source": [
        "# Dropout + Batch normalization\n",
        "increasing accuracy and idea of layer's arrangement taken from :\n",
        "https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eDXb5SyoZNcW"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv2D_1 = Conv2D(64 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.conv2D_2 = Conv2D(64 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.conv2D_3 = Conv2D(128 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.conv2D_4 = Conv2D(128 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.maxpool2D = MaxPooling2D()\n",
        "    self.dropout = Dropout(0.2)\n",
        "    self.dropout1 = Dropout(0.3)\n",
        "    self.dropout2 = Dropout(0.4)\n",
        "    self.dropout3 = Dropout(0.5)\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(128 , activation=\"relu\")\n",
        "    self.dense = Dense(10 , activation=\"softmax\")\n",
        "    self.batchnorm1 = BatchNormalization()\n",
        "    self.batchnorm2 = BatchNormalization()\n",
        "    self.batchnorm3 = BatchNormalization()\n",
        "    self.batchnorm4 = BatchNormalization()\n",
        "    self.batchnorm7 = BatchNormalization()\n",
        "\n",
        "\n",
        "  def call(self , x):\n",
        "    x = self.conv2D_1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.conv2D_2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.maxpool2D(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv2D_3(x)\n",
        "    x = self.batchnorm3(x)\n",
        "    x = self.conv2D_4(x)\n",
        "    x = self.batchnorm4(x)\n",
        "    x = self.maxpool2D(x)\n",
        "    x = self.dropout1(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.batchnorm7(x)\n",
        "    x = self.dropout3(x)\n",
        "    x = self.dense(x)\n",
        "    return x\n",
        "\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m8-j7qioapyk"
      },
      "outputs": [],
      "source": [
        "# Main loss function\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tHfEFk67bK76"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "test_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "test_acc = tf.keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1VdyENPcQS7"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CBJTrfGrcPUR"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train(images , labels):\n",
        "    with tf.GradientTape() as tape :\n",
        "      prediction = model(images)\n",
        "      loss = loss_function(y_true=labels , y_pred=prediction)\n",
        "\n",
        "    gradients = tape.gradient(loss , model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars= zip(gradients , model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_acc(labels, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i9BOGyHtd--u"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test(images , labels):\n",
        "  prediction = model(images)\n",
        "  loss = loss_function(y_true=labels , y_pred=prediction)\n",
        "  test_loss(loss)\n",
        "  test_acc(labels , prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck6RT0fVeVcA",
        "outputId": "bc621fd9-eca2-4901-eb0a-c629657083b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 Train Loss: 1.5578081607818604 Train Acc: 0.4380599856376648 Test loss: 1.2910444736480713 Test Acc: 0.5289999842643738\n",
            "epoch: 2 Train Loss: 1.1145458221435547 Train Acc: 0.6075800061225891 Test loss: 1.0006542205810547 Test Acc: 0.6455000042915344\n",
            "epoch: 3 Train Loss: 0.8965998888015747 Train Acc: 0.6862599849700928 Test loss: 0.8669480681419373 Test Acc: 0.6965000033378601\n",
            "epoch: 4 Train Loss: 0.7604389190673828 Train Acc: 0.7348600029945374 Test loss: 0.8365007638931274 Test Acc: 0.7067999839782715\n",
            "epoch: 5 Train Loss: 0.6516876220703125 Train Acc: 0.7751200199127197 Test loss: 0.752938449382782 Test Acc: 0.7369999885559082\n",
            "epoch: 6 Train Loss: 0.558782696723938 Train Acc: 0.8057799935340881 Test loss: 0.7206777334213257 Test Acc: 0.7541000247001648\n",
            "epoch: 7 Train Loss: 0.4710468649864197 Train Acc: 0.8385400176048279 Test loss: 0.6961079239845276 Test Acc: 0.7677000164985657\n",
            "epoch: 8 Train Loss: 0.392826110124588 Train Acc: 0.8668400049209595 Test loss: 0.7193120121955872 Test Acc: 0.7638000249862671\n",
            "epoch: 9 Train Loss: 0.3198816180229187 Train Acc: 0.8917199969291687 Test loss: 0.7535767555236816 Test Acc: 0.7738000154495239\n",
            "epoch: 10 Train Loss: 0.2540538012981415 Train Acc: 0.9157999753952026 Test loss: 0.7868900895118713 Test Acc: 0.7692999839782715\n",
            "epoch: 11 Train Loss: 0.19641442596912384 Train Acc: 0.9368399977684021 Test loss: 0.9054563045501709 Test Acc: 0.7645000219345093\n",
            "epoch: 12 Train Loss: 0.1455671340227127 Train Acc: 0.9540799856185913 Test loss: 0.8781552910804749 Test Acc: 0.7731000185012817\n"
          ]
        }
      ],
      "source": [
        "epochs = 12\n",
        "for epoch in range(epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_acc.reset_states()\n",
        "\n",
        "  #train\n",
        "  for images , labels in train_dataset :\n",
        "    train(images , labels)\n",
        "\n",
        "  # test\n",
        "  for images , labels in test_dataset :\n",
        "    test(images , labels)\n",
        "\n",
        "\n",
        "  print(\"epoch:\" , epoch + 1  ,\n",
        "        f\"Train Loss: {train_loss.result()}\" ,\n",
        "        f\"Train Acc: {train_acc.result()}\" ,\n",
        "        f\"Test loss: {test_loss.result()}\",\n",
        "        f\"Test Acc: {test_acc.result()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# best test accuracy :  0.773\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XQ7g7BBNdvLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ # decresing batch size from 32 to 20 , increased test accuracy during epochs\n",
        "+ # decreasing lr in RMSprop , helped a little .\n",
        "+ # increasing number of filters in conv2D , helped a little ."
      ],
      "metadata": {
        "id": "sLZWMgYJ2Rjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note :    \n",
        "after 5 epoch , test accuracy has been decreased .\n",
        "maybe for this reason :    <br>\n",
        "\n",
        " increasing the number of epochs can lead to overfitting, where the model becomes too focused on the training data and performs poorly on new, unseen data.\n"
      ],
      "metadata": {
        "id": "ZE9jS-3YdGyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"cifar10_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URWwu9bXWVKN",
        "outputId": "485b313f-954a-4a74-f3dc-997d8b5c7eeb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7857dc26c400>, because it is not built.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}