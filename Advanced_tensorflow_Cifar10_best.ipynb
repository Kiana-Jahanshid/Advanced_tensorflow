{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D708XPkBW6_m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense , Flatten , Conv2D , MaxPooling2D , Dropout , BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Gt4RYO54XWUn"
      },
      "outputs": [],
      "source": [
        "CIFAR10 = keras.datasets.cifar10\n",
        "\n",
        "(x_train , y_train) , (x_test , y_test ) = CIFAR10.load_data()\n",
        "x_train , x_test = x_train/255.0 , x_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g02f9IZWYIeW",
        "outputId": "1be7550e-8196-4afb-8c30-1bda52daa56e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "x_train[2].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL_ReR_QYZCT"
      },
      "source": [
        "# Adding batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vykaPmm7YW5g"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train , y_train)).batch(32).shuffle(buffer_size=1000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test , y_test)).batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D8vtlPejOAw",
        "outputId": "e710956c-3831-4fdf-8e79-48492562a0d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkEsoDp9TdO_"
      },
      "source": [
        "# Dropout + Batch normalization\n",
        "increasing accuracy and idea of layer's arrangement taken from :\n",
        "https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "eDXb5SyoZNcW"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv2D_1 = Conv2D(32 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.conv2D_2 = Conv2D(32 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.conv2D_3 = Conv2D(64 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.conv2D_4 = Conv2D(64 , (3,3) , activation=\"relu\" , padding=\"same\")\n",
        "    self.maxpool2D = MaxPooling2D()\n",
        "    self.dropout = Dropout(0.2)\n",
        "    self.dropout1 = Dropout(0.3)\n",
        "    self.dropout2 = Dropout(0.4)\n",
        "    self.dropout3 = Dropout(0.5)\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(256 , activation=\"relu\")\n",
        "    self.dense = Dense(10 , activation=\"softmax\")\n",
        "    self.batchnorm1 = BatchNormalization()\n",
        "    self.batchnorm2 = BatchNormalization()\n",
        "    self.batchnorm3 = BatchNormalization()\n",
        "    self.batchnorm4 = BatchNormalization()\n",
        "    self.batchnorm7 = BatchNormalization()\n",
        "\n",
        "\n",
        "  def call(self , x):\n",
        "    x = self.conv2D_1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.conv2D_2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.maxpool2D(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv2D_3(x)\n",
        "    x = self.batchnorm3(x)\n",
        "    x = self.conv2D_4(x)\n",
        "    x = self.batchnorm4(x)\n",
        "    x = self.maxpool2D(x)\n",
        "    x = self.dropout1(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.batchnorm7(x)\n",
        "    x = self.dropout3(x)\n",
        "    x = self.dense(x)\n",
        "    return x\n",
        "\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "m8-j7qioapyk"
      },
      "outputs": [],
      "source": [
        "# Main loss function\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "tHfEFk67bK76"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "test_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "test_acc = tf.keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1VdyENPcQS7"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CBJTrfGrcPUR"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train(images , labels):\n",
        "    with tf.GradientTape() as tape :\n",
        "      prediction = model(images)\n",
        "      loss = loss_function(y_true=labels , y_pred=prediction)\n",
        "\n",
        "    gradients = tape.gradient(loss , model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars= zip(gradients , model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_acc(labels, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "i9BOGyHtd--u"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test(images , labels):\n",
        "  prediction = model(images)\n",
        "  loss = loss_function(y_true=labels , y_pred=prediction)\n",
        "  test_loss(loss)\n",
        "  test_acc(labels , prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck6RT0fVeVcA",
        "outputId": "b8457874-dd28-4fa1-ebff-2d7365ac004b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 Train Loss: 1.2230784893035889 Train Acc: 0.5640199780464172 Test loss: 1.0165948867797852 Test Acc: 0.642799973487854\n",
            "epoch: 2 Train Loss: 0.7395774722099304 Train Acc: 0.7445399761199951 Test loss: 0.750623881816864 Test Acc: 0.7437999844551086\n",
            "epoch: 3 Train Loss: 0.5330339670181274 Train Acc: 0.8191800117492676 Test loss: 0.7402235269546509 Test Acc: 0.7601000070571899\n",
            "epoch: 4 Train Loss: 0.3924805521965027 Train Acc: 0.8687000274658203 Test loss: 0.9704635739326477 Test Acc: 0.7527999877929688\n",
            "epoch: 5 Train Loss: 0.285096675157547 Train Acc: 0.9066600203514099 Test loss: 1.1440701484680176 Test Acc: 0.7712000012397766\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_acc.reset_states()\n",
        "\n",
        "  #train\n",
        "  for images , labels in train_dataset :\n",
        "    train(images , labels)\n",
        "\n",
        "  # test\n",
        "  for images , labels in test_dataset :\n",
        "    test(images , labels)\n",
        "\n",
        "\n",
        "  print(\"epoch:\" , epoch + 1  ,\n",
        "        f\"Train Loss: {train_loss.result()}\" ,\n",
        "        f\"Train Acc: {train_acc.result()}\" ,\n",
        "        f\"Test loss: {test_loss.result()}\",\n",
        "        f\"Test Acc: {test_acc.result()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# best test accuracy :  0.7712\n",
        "\n"
      ],
      "metadata": {
        "id": "XQ7g7BBNdvLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note :    \n",
        "after 5 epoch , test accuracy has been decreased .\n",
        "maybe for this reason :    <br>\n",
        "\n",
        " increasing the number of epochs can lead to overfitting, where the model becomes too focused on the training data and performs poorly on new, unseen data.\n"
      ],
      "metadata": {
        "id": "ZE9jS-3YdGyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"cifar10_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URWwu9bXWVKN",
        "outputId": "7d08906c-b2bf-4fac-c162-37eedf531dce"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7e2f49314370>, because it is not built.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}